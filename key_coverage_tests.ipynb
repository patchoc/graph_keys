{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86ccda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from subprocess import run\n",
    "from subprocess import check_output\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import rdflib_hdt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052ee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "path_to_datasets_dir = '/Users/paulosh/Desktop/M2/stage_material/datasets/'\n",
    "path_to_sakey = '/Users/paulosh/Desktop/M2/stage_material/sakey/sakey.jar'\n",
    "class_keys = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab7a28",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a81b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms a text file into a dataframe, input: path to the text file\n",
    "def file_to_dataframe(path):\n",
    "    ds = []\n",
    "    with open(path,  'r', encoding='latin-1') as f:\n",
    "        lines = f.readlines()    \n",
    "        for line in lines:\n",
    "            ds.append(np.array(line.split(\".\\n\")[0].split(\"\\t\")))\n",
    "    \n",
    "    ds = pd.DataFrame(ds).dropna()\n",
    "    for i, row in ds.iterrows():    # removing the newline characters\n",
    "        ifor_val = ds[2][i]\n",
    "        if ds[2][i][-1:] == '\\n':\n",
    "            ifor_val = ds[2][i][:-1]\n",
    "        ds.at[i,2] = ifor_val\n",
    "    return ds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81ddeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "univ = file_to_dataframe(path_to_datasets_dir + 'University/DB_University')\n",
    "muse = file_to_dataframe(path_to_datasets_dir + 'Museum/DB_Museum')\n",
    "book = file_to_dataframe(path_to_datasets_dir + 'Book/DB_Book')\n",
    "moun = file_to_dataframe(path_to_datasets_dir + 'Mountain/DB_Mountain')\n",
    "albu = file_to_dataframe(path_to_datasets_dir + 'Album/DB_Album')\n",
    "scie = file_to_dataframe(path_to_datasets_dir + 'Scientist/DB_Scientist')\n",
    "acto = file_to_dataframe(path_to_datasets_dir + 'Actor/DB_Actor')\n",
    "film = file_to_dataframe(path_to_datasets_dir + 'Film/DB_Film')\n",
    "city = file_to_dataframe(path_to_datasets_dir + 'City/DB_City')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecd679",
   "metadata": {},
   "source": [
    "## Parsing the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9efd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments: path to the file we need to parse keys of, and nb of exceptions for SAkey\n",
    "def compute_keys(dataset1, nb_exceptions): \n",
    "    res = check_output(f'java -jar %s %s '%(path_to_sakey, dataset1) + str(nb_exceptions) , shell=True)\n",
    "    res = res.decode().split(\": \")[1][:-2]\n",
    "    res = res.split(\"\\n%s-almost keys:\" %(nb_exceptions - 1))\n",
    "    almost_keys = res[1]              \n",
    "    almost_keys_array = []\n",
    "    for i in almost_keys.split(\"], [\"):\n",
    "        splited = re.split(\"[\\[,\\] ]\", i)\n",
    "        almost_keys_array.append([])\n",
    "        for j in splited:\n",
    "            if j != '' and j != 'prop':\n",
    "                almost_keys_array[-1].append(j)\n",
    "    \n",
    "    if [] in almost_keys_array: almost_keys_array.remove([])            \n",
    "    \n",
    "    l = len(almost_keys_array)\n",
    "    if l > 0:           \n",
    "        class_keys[dataset1.split('DB_')[1]] = almost_keys_array\n",
    "        print(\"%i keys found\" %l)\n",
    "    else: print(\"no keys found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea2d84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the keys with a number of exceptions depending on the number of instances\n",
    "def compute_keys_threshold(dataset1, threshold): # threshold is a real number between 0 and 1\n",
    "    ds = file_to_dataframe(dataset1)\n",
    "    nb_exceptions = int(threshold*len(list(set(ds[0].values))))\n",
    "    print('number of exceptions = %s'%nb_exceptions)\n",
    "    res = check_output(f'java -jar %s %s '%(path_to_sakey, dataset1) + str(nb_exceptions) , shell=True)\n",
    "    res = res.decode().split(\": \")[1][:-2]\n",
    "    res = res.split(\"\\n%s-almost keys:\" %(nb_exceptions - 1))\n",
    "    almost_keys = res[1]              \n",
    "    almost_keys_array = []\n",
    "    for i in almost_keys.split(\"], [\"):\n",
    "        splited = re.split(\"[\\[,\\] ]\", i)\n",
    "        almost_keys_array.append([])\n",
    "        for j in splited:\n",
    "            if j != '' and j != 'prop':\n",
    "                almost_keys_array[-1].append(j)\n",
    "    \n",
    "    if [] in almost_keys_array: almost_keys_array.remove([])            \n",
    "    \n",
    "    l = len(almost_keys_array)\n",
    "    if l > 0:           \n",
    "        class_keys[dataset1.split('DB_')[1]] = almost_keys_array\n",
    "        print(\"%i keys found\" %l)\n",
    "    else: print(\"no keys found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380688eb",
   "metadata": {},
   "source": [
    "## Measuring their coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a6d484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_coverage(dataset, classe):\n",
    "    covered = not_covered = 0\n",
    "    for ins in list(set(dataset[0].values)): # for each instance,\n",
    "        good = bad = 0\n",
    "        triples = dataset[dataset[0] == ins] # we extract its properties \n",
    "        props = list(set(triples[1].values)) \n",
    "        \n",
    "        for key in class_keys[classe]:       # for each key of its type,\n",
    "            flag = True\n",
    "            for prop in key:                 # for each property in that key,\n",
    "                if prop not in props:        # if our instance does not have that property\n",
    "                    flag = False\n",
    "            \n",
    "            if flag: good+=1 \n",
    "            else: bad+=1                     # then that key does not cover it.\n",
    "        \n",
    "        if good == 0: not_covered+=1         # if 0 keys cover it, this instance is not covered\n",
    "        else: covered+=1\n",
    "            \n",
    "    ratio = round((covered/(covered + not_covered)) * 100, 4)\n",
    "    print(str(ratio) + '% of instances covered')        \n",
    "    return covered, not_covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a909da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys_and_coverage_measure(dataset1, nb_exceptions):\n",
    "    name = str(dataset1).split('DB_')[1]\n",
    "    print('\\n' + name)\n",
    "    dataset = file_to_dataframe(dataset1)\n",
    "    if type(nb_exceptions) == float: print('threshold = %s'%nb_exceptions); compute_keys_threshold(dataset1, nb_exceptions)\n",
    "    else: print('number of exceptions = %s'%(nb_exceptions-1)); compute_keys(dataset1, nb_exceptions)\n",
    "    key_coverage(dataset, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e891e",
   "metadata": {},
   "source": [
    "## Actual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91459d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "University\n",
      "threshold = 0.05\n",
      "number of exceptions = 517\n",
      "8 keys found\n",
      "84.42% of instances covered\n",
      "\n",
      "Museum\n",
      "threshold = 0.05\n",
      "number of exceptions = 91\n",
      "5 keys found\n",
      "99.3976% of instances covered\n",
      "\n",
      "Mountain\n",
      "threshold = 0.05\n",
      "number of exceptions = 820\n",
      "3 keys found\n",
      "53.0821% of instances covered\n",
      "\n",
      "Album\n",
      "threshold = 0.05\n",
      "number of exceptions = 4253\n",
      "3 keys found\n",
      "66.9609% of instances covered\n"
     ]
    }
   ],
   "source": [
    "k = 0.05 # set the threshold, or number of exceptions\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'University/DB_University', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Museum/DB_Museum', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Book/DB_Book', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Mountain/DB_Mountain', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Album/DB_Album', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Scientist/DB_Scientist', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Actor/DB_Actor', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'Film/DB_Film', k)\n",
    "\n",
    "keys_and_coverage_measure(path_to_datasets_dir + 'City/DB_City', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa80ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
